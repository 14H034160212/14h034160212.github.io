<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link href="https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300" rel="stylesheet" type="text/css">
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>Qiming Bao - CV</title>
<style type="text/css">
  body { margin-top: 0px; }
  #containter{
    font-size: 11pt;
    font-family: Calibri, Candara, Segoe, "Segoe UI", Optima, Arial, sans-serif;
    line-height: 12pt;
    margin-left:auto;
    margin-right:auto;
  }
  @media screen{ #containter{ width: 680px; } }

  .grey { color: #666666; }
  #header{
    width:100%;
    text-align: left;
    border-bottom: solid 1px #000;
  }

  a:link, a:visited { color: #666; text-decoration: underline; }

  p { margin:0; margin-left:1em; }
  .p_more_indent { margin:0; margin-left:2em; }
  .clear { clear:both; }

  h1{
    margin-bottom: 10pt;
    font-family: 'Open Sans Condensed', sans-serif;
    font-size: 24pt;
    line-height: 0pt;
    letter-spacing: 1px;
    text-align: center;
  }
  h2{
    font-family: 'Open Sans', sans-serif;
    margin-top: 0.7em;
    margin-bottom: 0.4em;
    font-size: 14pt;
    color: black;
    font-weight: 900;
  }
  ul{ margin-top: 2pt; margin-bottom: 6pt; }
  li{ margin-top: 0.25em; }

  .location{ float:right; font-size: 10pt; color: #000; }
  .date{
    font-family: 'Open Sans', sans-serif;
    float:right;
    font-size: 10pt;
    color:#000;
  }

  .section_sub{ margin-bottom: 10px; }

  .activity_header{
    font-family: 'Open Sans', sans-serif;
    color: #000;
  }
  .activity_header strong{
    font-family: 'Open Sans', sans-serif;
    font-size: 12pt;
    color: #000;
  }
  .activity_header_sub{ font-size: 10pt; color: #000; }

  .header-left { width: 38%; float:left; }
  .header-right { width: 40%; float:right; text-align: right; }
  .header-center { width: 22%; float:right; }

  /* ✅ Google Scholar 单行关键：不换行 + 略微缩小显示 */
  .one-line { white-space: nowrap; }
  .one-line a { white-space: nowrap; font-size: 10pt; }

  /* 让长链接显示更紧凑，仍保持可点击 */
  .link-compact { font-size: 10pt; }

  /* 小字信息区 */
  .meta { font-size: 10pt; line-height: 12pt; }
</style>
</head>

<body>
<div id="containter">

  <!-- Header -->
  <div id="header">
    <div class="header-left meta">
      <br><br>
      Last Name: BAO<br>
      First Name: QIMING<br>
      <span class="one-line">Google Scholar:
        <a class="link-compact" href="https://scholar.google.com/citations?user=t-PqsgcAAAAJ&hl=en">
          scholar.google.com/citations?user=t-PqsgcAAAAJ
        </a>
      </span><br>
    </div>

    <div class="header-right meta">
      <br><br>
      Phone: (+64) 225120099<br>
      Email: <a href="mailto:bqmbill714@gmail.com">bqmbill714@gmail.com</a><br>
      GitHub: <a class="link-compact" href="https://github.com/14H034160212">github.com/14H034160212</a><br>
    </div>

    <div class="header-center">
      <h1>QIMING BAO</h1>
    </div>

    <br>
    <div class="clear"></div>
  </div>

  <div id="content">

    <!-- Summary -->
    <div class="section">
      <h2>Summary</h2>
      <div class="section_sub">
        <p>
          AI Researcher/Engineer with 5+ years of hands-on experience building and deploying
          Machine Learning / Deep Learning systems, with a strong focus on large language and vision-language models.
          Experienced in long-context / document understanding (LayoutLMv3, ERNIE-LayoutX, Qwen-VL),
          efficient training and inference (DeepSpeed, PEFT/adapters, FP16, FlashAttention2, GPTQ int4),
          and production-oriented evaluation loops (data → iteration → regression).
          Strong publication record in logical reasoning and robust NLP, including ACL Findings (2024) and AAAI (2025).
        </p>
      </div>
    </div>

    <!-- Skills -->
    <div class="section">
      <h2>Core Skills</h2>
      <div class="section_sub">
        <ul>
          <li><em>LLM/VLM Engineering</em>: Instruction tuning, prompt engineering, multi-task pretraining, adapters/LoRA, quantization (GPTQ int4), evaluation & regression.</li>
          <li><em>Efficiency & Scale</em>: DeepSpeed, FP16, FlashAttention2; memory-aware training and deployment optimizations.</li>
          <li><em>Document AI</em>: Layout-aware multimodal modeling, word-patch alignment, long-context mechanisms (sliding window, global attention masks).</li>
          <li><em>Programming</em>: Python, Golang, Java, SQL; Linux; Git.</li>
          <li><em>Data/Platforms</em>: Neo4j, MySQL, SQL Server; MLflow, Weights & Biases; AWS/GCP/Azure.</li>
          <li><em>AI App Integration</em>: LangChain, Dify.ai, CrewAI, OpenAI APIs.</li>
        </ul>
      </div>
    </div>

    <!-- Experience -->
    <div class="section">
      <h2>Work & Project Experience</h2>

      <!-- Xtracta -->
      <div class="section_sub">
        <span class="activity_header"><strong>Xtracta — Enhancing Long-Context in Large Multimodal Document Models</strong></span>
        <span class="location">Auckland, New Zealand</span><br>
        <span class="activity_header activity_header_sub">Artificial Intelligence Researcher/Engineer</span>
        <span class="date">07/2022 – Present</span>

        <ul>
          <li>Extended effective max sequence length for LayoutLMv3 and ERNIE-LayoutX from 512 → 4096 using sliding-window attention and Longformer-style global attention masks, improving extraction quality on XFUND, FUNSD, and internal datasets without significant GPU memory growth.</li>
          <li>Reproduced LayoutLMv3 multi-task multimodal pretraining (masked language modeling, masked image modeling, word–patch alignment) despite incomplete upstream open-source availability, enabling internal experimentation and controlled ablations.</li>
          <li>Integrated DeepSpeed and adapter-based fine-tuning into ERNIE-LayoutX/LayoutLMv3 to reduce training cost, shrink deployable artifact size, and accelerate iteration cycles.</li>
          <li>Integrated FlashAttention2 into self-attention modules, reducing peak training GPU memory by up to ~50% under FP16 for targeted workloads.</li>
          <li>Improved robustness of line alignment for document extraction via affine-transform data augmentation and training-time perturbations.</li>
          <li>Continual training pipeline for Qwen-VL (8B class) combining PEFT adapters + FlashAttention2 + GPTQ int4 for cost-efficient training on multi-GPU hardware.</li>
          <li>Added page embeddings for multi-page documents (Qwen-VL / ERNIE-LayoutX), improving performance on page-consistent fields (e.g., supplier/bank names) by 15%+ on targeted evaluation sets.</li>
          <li>Successfully prepared and supported R&D Tax Incentive (RDTI) applications (2022/2023), aligning technical deliverables with eligible R&D criteria.</li>
        </ul>
      </div>

      <!-- Kerrio.ai -->
      <div class="section_sub">
        <span class="activity_header"><strong>Kerrio.ai — Cognitive ChatGPT-style Product MVP (GenAI)</strong></span>
        <span class="location">Auckland, New Zealand</span><br>
        <span class="activity_header activity_header_sub">Head of AI / Lead Engineer (Hands-on MVP Builder)</span>
        <!-- NOTE: 你原文写 11/25 – Now（未来时间）。这里按常见写法改为 11/2024 – Present；如确实为 2025，请自行改回。 -->
        <span class="date">11/2024 – Present</span>
        <ul>
          <li>Owned end-to-end delivery of a coaching-oriented conversational AI MVP, from use-case definition and prototyping to iterative releases based on user feedback.</li>
          <li>Built an evaluation & iteration loop (data collection → prompt/logic refinement → regression checks) to achieve predictable quality gains across releases.</li>
          <li>Worked with stakeholders to translate requirements into technical milestones and deliver in agile sprint cycles.</li>
          <li>Project reference: <a href="https://github.com/14H034160212/gptcoaching_mi_training">github.com/14H034160212/gptcoaching_mi_training</a></li>
        </ul>
      </div>

      <!-- PhD -->
      <div class="section_sub">
        <span class="activity_header"><strong>University of Auckland — Large Language Models & Logical Reasoning (PhD)</strong></span>
        <span class="location">Auckland, New Zealand</span><br>
        <span class="activity_header activity_header_sub">Research Project Leader / Developer</span>
        <span class="date">02/2020 – 09/2025</span>
        <ul>
          <li>Led research direction on logical reasoning within the Strong AI Lab program (Grant No. 5000675; NZD 9.6M total program funding).</li>
          <li>Developed an LLM-based self-reinforcement framework for generating and evaluating explanations; accepted by AAAI (2025) and AGI@ICLR (2024). <a href="https://ojs.aaai.org/index.php/AAAI/article/view/35164">Paper</a>, <a href="https://github.com/Strong-AI-Lab/Explanation-Generation">Code</a>.</li>
          <li>Developed AMR-LDA and achieved #1 on ReClor leaderboard; published at Findings of ACL (2024). <a href="https://aclanthology.org/2024.findings-acl.353/">Paper</a>, <a href="https://github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning">Code</a>, <a href="https://huggingface.co/qbao775/AMR-LE-DeBERTa-V2-XXLarge-Contraposition">Weights</a>.</li>
          <li>Evaluated robustness of LLMs under out-of-distribution task variations; highlighted performance sensitivity to minor input structure changes (IJCAI/ICONIP-related work).</li>
          <li>Built reasoning datasets (e.g., PARARULE-Plus, AbductionRules) and released open-source code/data used by multiple community collections and evaluation efforts.</li>
        </ul>
      </div>

      <!-- AIIT -->
      <div class="section_sub">
        <span class="activity_header"><strong>AIIT, Peking University — Abstract Extraction & Multi-Turn Dialogue System</strong></span>
        <span class="location">Hangzhou, China</span><br>
        <span class="activity_header activity_header_sub">R&D Engineer</span>
        <span class="date">11/2019 – 02/2020</span>
        <ul>
          <li>Built a robot-based pipeline for abstract extraction, text segmentation, topic prediction, and multi-turn QA; delivered a reusable API for meeting-record processing workflows.</li>
        </ul>
      </div>

      <!-- PDH -->
      <div class="section_sub">
        <span class="activity_header"><strong>Precision Driven Health — Online Medical Chatbot System (HHH)</strong></span>
        <span class="location">Auckland, New Zealand</span><br>
        <span class="activity_header activity_header_sub">Research Project Leader / Developer</span>
        <span class="date">11/2018 – 04/2019</span>
        <ul>
          <li>Developed HBAM (DL + Knowledge Graph) for medical text similarity; released code (90+ stars) and published at ACSW (2020), 80+ citations: <a href="https://arxiv.org/abs/2002.03140">Paper</a>.</li>
          <li>Built a medical PII detection & redaction pipeline (spaCy-based) to support privacy-preserving training/inference; backed by AUT Venture investment (NZD 20,000).</li>
        </ul>
      </div>

    </div>

    <!-- Education -->
    <div class="section">
      <h2>Education</h2>

      <div class="section_sub">
        <span class="activity_header"><strong>University of Auckland (UoA)</strong></span>
        <span class="location">Auckland, New Zealand</span><br>
        <span class="activity_header activity_header_sub">Ph.D. in Computer Science (Supervisors: Prof. Michael Witbrock, Assoc Prof. Jiamou Liu)</span>
        <span class="date">02/2020 – 09/2025</span>
        <ul>
          <li>DAAD AINeT Fellow 2025 (NLP): <a href="https://www.daad.de/en/the-daad/postdocnet/fellows/fellows/#BaoQiming">Link</a></li>
          <li>Graduate Teaching Assistant / Research Assistant</li>
        </ul>
      </div>

      <div class="section_sub">
        <span class="activity_header"><strong>University of Auckland (UoA)</strong></span>
        <span class="location">Auckland, New Zealand</span><br>
        <span class="activity_header activity_header_sub">BSc (Hons) in Computer Science (First Class), GPA: 7/9</span>
        <span class="date">07/2018 – 09/2019</span>
        <ul>
          <li>Precision Driven Health Summer Research Scholarship</li>
        </ul>
      </div>

    </div>

    <!-- Publications -->
    <div class="section">
      <h2>Selected Publications</h2>
      <div class="section_sub">
        <ul>
          <li><u>Qiming Bao</u>, Juho Leinonen, Alex Yuxuan Peng, Wanjun Zhong, Tim Pistotti, Alice Huang, Paul Denny, Michael Witbrock, Jiamou Liu.
            <em>Exploring Iterative Enhancement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models</em>,
            <b>AAAI (2025)</b>.
          </li>
          <li><u>Qiming Bao</u>, Alex Peng, Zhenyun Deng, Wanjun Zhong, Gaël Gendron, Neşet Tan, Nathan Young, Yang Chen, Yonghua Zhu, Michael Witbrock, Jiamou Liu.
            <em>Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning</em>,
            <b>Findings of ACL (2024)</b>.
          </li>
          <li><u>Qiming Bao</u>, Gaël Gendron, Alex Peng, Neset Tan, Michael Witbrock, Jiamou Liu.
            <em>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning</em>,
            <b>ICONIP (2024)</b>.
          </li>
          <li>Gaël Gendron, <u>Qiming Bao</u>, Michael Witbrock, Gillian Dobbie.
            <em>Large Language Models Are Not Strong Abstract Reasoners</em>,
            <b>IJCAI (2024)</b>.
          </li>
          <li>Lin Ni, <u>Qiming Bao</u>, Xiaoxuan Li, Qianqian Qi, Paul Denny, Jim Warren, Michael Witbrock, Jiamou Liu.
            <em>DeepQR: Neural-based Quality Ratings for Learnersourced Multiple-Choice Questions</em>,
            <b>AAAI (2022)</b>.
          </li>
          <li><u>Qiming Bao</u>, Lin Ni, Jiamou Liu.
            <em>HHH: An Online Medical Chatbot System based on Knowledge Graph and Hierarchical Bi-Directional Attention</em>,
            <b>ACSW (2020)</b>.
          </li>
        </ul>
      </div>
    </div>

  </div><!-- content -->
</div><!-- container -->
</body>
</html>
