
<!DOCTYPE html>
<html>
<head>
    <title>Qiming Bao's Homepage</title>
    <link rel="icon" type="image/jpeg" href="./qiming.jpg">
    <style>
        body { font-family: Arial, sans-serif; margin: 0; padding: 0; }
        .container { width: 80%; margin: 0 auto; }
		.location{
		  float:right;
		  font-size: 10pt;
		  color: #000;
		}
		.date {
		  font-family: 'Open Sans', sans-serif;
		  float:right;
		  font-size: 10pt;
		  color:#878787;
		}
		.section_sub{
		  margin-bottom:10px;
		}
		.activity_header {
		  font-family: 'Open Sans', sans-serif;
		  color: #666;
		}
		 .header-left {
		  width: 36%;
		  float:left;
		}
		.header-right {
		  width: 42%;
		  float:right;
		  text-align: right;
		}
		.header-center {
		  width: 22%;
		  float:right;
		}
		.small-caps {
		  font-variant: small-caps;
		}
		.activity_header strong {
		  font-family: 'Open Sans', sans-serif;
		  font-size: 12pt;
		  /*font-weight: 600;*/
		  color: #000; /*#761F1F;*/
		}
		.activity_header stronger {
		  font-family: 'Open Sans', sans-serif;
		  font-size: 14pt;
		  /*font-weight: 600;*/
		  color: #000; /*#761F1F;*/
		}
		.activity_header_sub {
		  font-size: 10pt;
		}
        header { background-color: #f1f1f1; padding: 20px; text-align: center; }
        section { margin: 20px 0; }
        h2 { color: #333; }
        ul { list-style-type: none; padding: 0; }
        li { margin-bottom: 10px; }
    </style>
</head>
<body>
    <header>
        <h1>Qiming Bao</h1>
        <p>AI Researcher and Engineer @ <A href="https://xtracta.com/"><FONT face="Bitstream Vera Sans">Xtracta</FONT></A> </p>
	<p>LLMs Reasoning + Long Context VLMs | Intelligent Document Processing</p>
	<p>Ex-<A href="https://www.aiit.org.cn/p_enPage"><FONT face="Bitstream Vera Sans">AIIT, Peking University</FONT></A>, MSRA, Samsung AI UK</p>
	<p>CS Ph.D. graduated from the <A href="https://www.ai.ac.nz/"><FONT face="Bitstream Vera Sans">Strong AI Lab</FONT></A>, <A href="https://www.naoinstitute.auckland.ac.nz/"><FONT face="Bitstream Vera Sans">NAOInstitute</FONT></A>, University of Auckland, New Zealand</p>
		<!--<a href="https://profiles.auckland.ac.nz/qiming-bao" target="_blank" style="text-decoration: none;">
			<img src="https://img.shields.io/badge/-Homepage-purple?style=flat-square&logo=homepage&logoColor=white" alt="Homepage">
		</a>&nbsp;&nbsp;-->
		
		<a href="https://www.linkedin.com/in/qiming-bill-bao-773757166/" target="_blank" style="text-decoration: none;">
			<img src="https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white" alt="LinkedIn">
		</a>&nbsp;&nbsp;
		
		<a href="https://github.com/14H034160212" target="_blank" style="text-decoration: none;">
			<img src="https://img.shields.io/badge/-GitHub-black?style=flat-square&logo=GitHub&logoColor=white" alt="GitHub">
		</a>&nbsp;&nbsp;

		<a href="mailto:bqmbill714@gmail.com" target="_blank" style="text-decoration: none;">
			<img src="https://img.shields.io/badge/-Gmail-red?style=flat-square&logo=Gmail&logoColor=white" alt="Gmail">
		</a>&nbsp;&nbsp;

		<a href="https://scholar.google.com/citations?user=t-PqsgcAAAAJ&hl=en" target="_blank" style="text-decoration: none;">
			<img src="https://img.shields.io/badge/-Google%20Scholar-blue?style=flat-square&logo=Google%20Scholar&logoColor=white" alt="Google Scholar">
		</a>&nbsp;&nbsp;
		
		<a href="https://dblp.org/pid/126/9037-1.html" target="_blank" style="text-decoration: none;">
			<img src="https://img.shields.io/badge/-DBLP-yellow?style=flat-square&logo=DBLP&logoColor=white" alt="DBLP">
		</a>&nbsp;&nbsp;

	    	<a href="https://orcid.org/my-orcid?orcid=0000-0002-1000-7383" target="_blank" style="text-decoration: none;">
			<img src="https://img.shields.io/badge/-ORCID-green?style=flat-square&logo=ORCID&logoColor=white" alt="DBLP">
		</a>&nbsp;&nbsp;

		<a href="https://twitter.com/qiming_bao" target="_blank" style="text-decoration: none;">
			<img src="https://img.shields.io/badge/-Twitter-blue?style=flat-square&logo=X&logoColor=white&link=https://x.com/qiming_bao" alt="Twitter">
		</a>&nbsp;&nbsp;
		
		<a href="https://14h034160212.github.io/cv.html" target="_blank" style="text-decoration: none;">
			<img src="https://img.shields.io/badge/-CV-brown?style=flat-square&logo=CV&logoColor=white" alt="CV">
		</a>&nbsp;&nbsp;
	    	<a href="https://14h034160212.github.io/包启明.pdf" target="_blank" style="text-decoration: none;">
			<img src="https://img.shields.io/badge/-简历-purple?style=flat-square&logo=简历&logoColor=white" alt="简历">
		</a>
    </header>
    <div class="container">
		<section>
            <h2>Personal Details</h2>
            <ul>
				Qiming is an AI researcher and engineer at <A href="https://xtracta.com/"><FONT face="Bitstream Vera Sans">Xtracta</FONT></A> in Auckland, New Zealand, where he used the PEFT adapter, Flash-Attention 2 and GPTQ int4 quantization for continual training on the large vision language model Qwen2-VL-7B for intelligent document processing on an A4090 GPU. He also investigated and implemented alternative attention mechanisms to extend the effective sequence length in multi-modal document processing models such as LayoutLMv3 and ERNIE-LayoutX. He replicated the multi-task, multimodal pre-training code for LayoutLMv3, which Microsoft did not open source, including masked language modeling, masked image modeling, and word-patch alignment. He integrated DeepSpeed and adapters into ERNIE-LayoutX and LayoutLMv3, which can reduce training costs, result in a smaller model size, and make it easier to deploy in the production environment. He successfully applied for the Research & Development Tax Incentive (RDTI) grants from Callaghan Innovation (New Zealand's Innovation Agency) for both 2022 and 2023, each offering a tax credit equal to 15% of eligible R&D expenditure. This credit can be utilised to reduce the income tax payable by the company. Prior to this role, he worked as a research and development engineer in <A href="https://www.aiit.org.cn/p_enPage"><FONT face="Bitstream Vera Sans">AIIT</FONT></A> at Peking University, where he focused on automatic abstract generation and GPT-2 based dialog chatbot development. Qiming also has a great deal of teaching experience, having worked as a teaching assistant for three years. He earned a Bachelor of Science (Honours) in Computer Science (First Class) from the University of Auckland and completed a Summer Research Internship with Scholarship in <A href="https://precisiondrivenhealth.com/"><FONT face="Bitstream Vera Sans">Precision Driven Health & Orion Health</FONT></A>. In addition, he was selected as one of ten students to participate in the <A href="https://www.hinz.org.nz/news/461570/An-online-system-for-answering-medical-questions.htm"><FONT face="Bitstream Vera Sans">Summer Research Program</FONT></A> funded by Precision Driven Health & Orion Health, where the main topic was developing a Medical Chatbot based on Deep Learning and Knowledge Graph. <br /><br />
		    		Qiming Bao is a Ph.D. graduated from the <A href="https://www.ai.ac.nz/sail/"><FONT face="Bitstream Vera Sans">Strong AI Lab</FONT></A>, <A href="https://www.naoinstitute.auckland.ac.nz/"><FONT face="Bitstream Vera Sans">NAOInstitute</FONT></A>, University of Auckland, New Zealand, supervised by Professor <A href="https://profiles.auckland.ac.nz/m-witbrock"><FONT face="Bitstream Vera Sans">Michael Witbrock</FONT></A> and Associate Professor <A href="https://profiles.auckland.ac.nz/jiamou-liu"><FONT face="Bitstream Vera Sans">Jiamou Liu</FONT></A>. His research interests include natural language processing and reasoning. He has over five years of research and development experience, and has published several papers in top conferences in the fields of AI/NLP/Reasoning, including <B>ACL</B>, <B>AAAI</B>, <B>IJCAI</B>, <B>ICLR</B>, <B>EACL</B>, <B>LLM@IJCAI</B>, <B>AGI@ICLR</B> and <B>IJCLR-NeSy</B>. His method named <B>AMR-LDA</B> (GPT-4 + AMR-LDA Prompt Augmentation) has achieved the <B>#1</B> ranking on a one of the most challenged logical reasoning reading comprehension leaderboards (<A href="https://eval.ai/web/challenges/challenge-page/503/leaderboard/1347"><FONT face="Bitstream Vera Sans">ReClor</FONT></A>) and we are the first group scored above 90% on the hidden test set around the world. Two of his logical reasoning datasets called <A href="https://github.com/Strong-AI-Lab/PARARULE-Plus"><FONT face="Bitstream Vera Sans">PARARULE-Plus</FONT></A> and <A href="https://github.com/Strong-AI-Lab/AbductionRules"><FONT face="Bitstream Vera Sans">AbductionRules</FONT></A> have been collected by <A href="https://www.logitorch.ai/"><FONT face="Bitstream Vera Sans">LogiTorch</FONT></A>, <A href="https://github.com/FreedomIntelligence/ReasoningNLP"><FONT face="Bitstream Vera Sans">ReasoningNLP</FONT></A>, <A href="https://github.com/zjunlp/Prompt4ReasoningPapers"><FONT face="Bitstream Vera Sans">Prompt4ReasoningPapers</FONT></A>, <A href="https://github.com/openai/evals/pull/651"><FONT face="Bitstream Vera Sans">OpenAI/Evals</FONT></A>, <A href="https://github.com/MLGroupJLU/LLM-eval-survey"><FONT face="Bitstream Vera Sans">A Survey on Evaluation of Large Language Models</FONT></A> and <A href="https://github.com/spcl/x1"><FONT face="Bitstream Vera Sans">Reasoning Language Models: A Blueprint</FONT></A>. Qiming has given public guest talks and academic visit at <A href="https://youtu.be/nfNbSZPY4EU"><FONT face="Bitstream Vera Sans">Microsoft Research Asia</FONT></A>, <A href="https://youtu.be/0ZkayBD3WVY"><FONT face="Bitstream Vera Sans">Samsung AI Center Cambridge UK</FONT></A>, <A href="https://youtu.be/ZzCpq5gXQto"><FONT face="Bitstream Vera Sans">IEEE Vehicular Technology Society</FONT></A>, <A href="https://www.zjukg.org/"><FONT face="Bitstream Vera Sans">ZJU-NLP Group, Zhejiang University</FONT></A>, <A href="https://14h034160212.github.io/Invitation_UoM_NLP_Reading_Group.pdf"><FONT face="Bitstream Vera Sans">The University of Melbourne</FONT></A>, <A href="https://14h034160212.github.io/中科院自动化所论坛邀请函.png"><FONT face="Bitstream Vera Sans">Institute of Automation, Chinese Academy of Sciences</FONT></A>, <A href="https://14h034160212.github.io/国际青年学者论坛人工智能研究院分论坛时间表.pdf"><FONT face="Bitstream Vera Sans">Shenzhen MSU-BIT University</FONT></A>, <A href="https://www.cics.umass.edu/about/directory/andrew-lan"><FONT face="Bitstream Vera Sans">University of Massachusetts - Amherst</FONT></A>, <A href="https://14h034160212.github.io/Presentation_Confirmation_Penn_State_University_University_of_Auckland_Online_Workshop.pdf"><FONT face="Bitstream Vera Sans">Penn State University</FONT></A>, Logic and AI Seminar@Peking University & Tsinghua University, Max Planck Institute For Software Systems and Technical University of Munich on his main research topic, "Natural Language Processing and Reasoning".
            </ul>
        </section>
        <section>
            <h2>Education</h2>
			<li>Ph.D. of Computer Science, University of Auckland (2020 - 2024)</li>
			<li>B.Sc. (Honours) of Computer Science (First Class), University of Auckland (2018 - 2019)</li>
			<li>Bachelor of Engineering, China Jiliang University (2014 - 2018)</li>
            
        </section>
        <section>
            <h2>Research Interests</h2>
            <p>AI/DL, NLP, LLMs/VLMs, Neural-Symbolic AI, Reasoning, Multimodal Document AI, Intelligent Document Processing</p>
        </section>
        <section>
            <h2>Publications</h2>
			<li><u>Qiming Bao</u>. <em>Developing And Assessing Language Models For Logical Reasoning Over Natural Language</em> <b>[<A href="https://researchspace.auckland.ac.nz/items/c137d667-49c5-4fa4-9da0-c2e74e3d426f"><FONT face="Bitstream Vera Sans">PhD Thesis Link</FONT></A>] </b></li>
            <li><u>Qiming Bao</u>, Juho Leinonen, Alex Yuxuan Peng, Wanjun Zhong, Tim Pistotti, Alice Huang, Paul Denny, Michael Witbrock, Jiamou Liu. <em>Exploring Iterative Enhancement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models</em>, <b><A href="https://eaai-conf.github.io/year/eaai-25.html"><FONT face="Bitstream Vera Sans">Proceedings of the AAAI Conference on Artificial Intelligence (2025)</FONT></A> [<A href="https://ojs.aaai.org/index.php/AAAI/article/view/35164"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>] [<A href="https://github.com/Strong-AI-Lab/Explanation-Generation"><FONT face="Bitstream Vera Sans">Source code</FONT></A>]</b></li>
			<li><u>Qiming Bao</u>, Alex Peng, Zhenyun Deng, Wanjun Zhong, Gaël Gendron, Neşet Tan, Nathan Young, Yang Chen, Yonghua Zhu, Michael Witbrock, Jiamou Liu. <em>Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning.</em>, the Findings of <b><A href="https://2024.aclweb.org/"><FONT face="Bitstream Vera Sans">ACL-24</FONT></A> [<A href="https://eval.ai/web/challenges/challenge-page/503/leaderboard/1347"><FONT face="Bitstream Vera Sans">#1 on the ReClor Leaderboard</FONT></A>] [<A href="https://aclanthology.org/2024.findings-acl.353/"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>] [<A href="https://github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning"><FONT face="Bitstream Vera Sans">Source code</FONT></A>]</b></li>
			<li><u>Qiming Bao</u>, Juho Leinonen, Alex Yuxuan Peng, Wanjun Zhong, Tim Pistotti, Alice Huang, Paul Denny, Michael Witbrock, Jiamou Liu. <em>Exploring Iterative Enhancement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models</em>, <b><A href="https://agiworkshop.github.io/"><FONT face="Bitstream Vera Sans">AGI@ICLR 2024</FONT></A> [<A href="http://arxiv.org/abs/2309.10444"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>] [<A href="https://github.com/Strong-AI-Lab/Explanation-Generation"><FONT face="Bitstream Vera Sans">Source code</FONT></A>]</b></li>
			<li><u>Qiming Bao</u>, Gaël Gendron, Alex Peng, Neset Tan, Michael Witbrock, Jiamou Liu. <em>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning.</em>, <b><A href="https://iconip2024.org/"><FONT face="Bitstream Vera Sans">ICONIP-24</FONT></A> [<A href="https://arxiv.org/abs/2310.09430"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>] [<A href="https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning"><FONT face="Bitstream Vera Sans">Source code</FONT></A>]</b></li>
			<li><u>Qiming Bao</u>, Gaël Gendron, Alex Peng, Neset Tan, Michael Witbrock, Jiamou Liu. <em>A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks.</em>, <b><A href="https://bigmodel.ai/llm-ijcai23"><FONT face="Bitstream Vera Sans">LLM@IJCAI'23</FONT></A> [<A href="https://arxiv.org/abs/2310.09430"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>] [<A href="https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning"><FONT face="Bitstream Vera Sans">Source code</FONT></A>]</b></li>
			<li><u>Qiming Bao</u>, Alex Peng, Zhenyun Deng, Wanjun Zhong, Gaël Gendron, Neşet Tan, Nathan Young, Yang Chen, Yonghua Zhu, Michael Witbrock, Jiamou Liu. <em>Enhancing Logical Reasoning of Large Language Models through Logic-Driven Data Augmentation.</em>, <b><A href="https://bigmodel.ai/llm-ijcai23"><FONT face="Bitstream Vera Sans">LLM@IJCAI'23</FONT></A> [<A href="https://eval.ai/web/challenges/challenge-page/503/leaderboard/1347"><FONT face="Bitstream Vera Sans">#1 on the ReClor Leaderboard</FONT></A>] [<A href="https://arxiv.org/abs/2305.12599"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>] [<A href="https://github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning"><FONT face="Bitstream Vera Sans">Source code</FONT></A>]</b></li>
			<!--<li><u>Qiming Bao</u>, Alex Peng, Zhenyun Deng, Wanjun Zhong, Neset Tan, Nathan Young, Yang Chen, Yonghua Zhu, Michael Witbrock, Jiamou Liu. <em>Contrastive Learning with Logic-driven Data Augmentation for Logical Reasoning over Text.</em>, <b>arXiv preprint arXiv:2305.12599 (2023)</b></li>-->
			<li><u>Qiming Bao</u>, Alex Peng, Tim Hartill, Neset Tan, Zhenyun Deng, Michael Witbrock, Jiamou Liu. <em>Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation</em>, <b><A href="http://ceur-ws.org/Vol-3212/"><FONT face="Bitstream Vera Sans">IJCLR-NeSy-22</FONT></A> [<A href="https://arxiv.org/abs/2207.14000"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>] [<A href="https://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language"><FONT face="Bitstream Vera Sans">Source code and dataset</FONT></A>] [<A href="http://ilp.doc.ic.ac.uk/ijclr22_videos/NeSy%20Session%205%20-%20Thursday%2029th%20-%2014_40%20-%2015_50%20(BST)%20includes%20NeSy%20Invited%20Talk%20William%20Cohen.mp4"><FONT face="Bitstream Vera Sans">Presentation recording</FONT></A>]</b></li>
			<li>Nathan Young, <u>Qiming Bao</u>, Joshua Ljudo Bensemann, Michael J. Witbrock. <em>AbductionRules: Training Transformers to Explain Unexpected Inputs</em>, the Findings of <b><A href="https://www.2022.aclweb.org/"><FONT face="Bitstream Vera Sans">ACL-22</FONT></A> [<A href="https://aclanthology.org/2022.findings-acl.19/"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>] [<A href="https://github.com/Strong-AI-Lab/AbductionRules"><FONT face="Bitstream Vera Sans">Source code</FONT></A>]</b></li>
			<li>Gaël Gendron, <u>Qiming Bao</u>, Michael Witbrock, Gillian Dobbie. <em>Large Language Models Are Not Strong Abstract Reasoners</em>, <b><A href="https://ijcai24.org/"><FONT face="Bitstream Vera Sans">IJCAI 2024</FONT></A> [<A href="https://arxiv.org/abs/2305.19555"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>] [<A href="https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning"><FONT face="Bitstream Vera Sans">Source code and evaluation platform</FONT></A>]</b></li>
			<li>Lin Ni, <u>Qiming Bao</u>, Xiaoxuan Li, Qianqian Qi, Paul Denny, Jim Warren, Michael Witbrock, Jiamou Liu. <em>DeepQR: Neural-based Quality Ratings for Learnersourced Multiple-Choice Questions</em>, <b><A href="https://aaai.org/conference/aaai/aaai-22/eaai-22/"><FONT face="Bitstream Vera Sans">Proceedings of the AAAI Conference on Artificial Intelligence (2022)</FONT></A> [<A href="https://ojs.aaai.org/index.php/AAAI/article/view/21562"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>]</b></li>
			<li>Qianqian Qi, <u>Qiming Bao</u>*, Alex Yuxuan Peng, Jiamou Liu, Michael Witbrock. <em>Enhancing Data Augmentation with Knowledge-Enriched Data Generation via Dynamic Prompt-Tuning Method</em>, <b><A href="https://ieeexplore.ieee.org/xpl/conhome/10649807/proceeding"><FONT face="Bitstream Vera Sans">IJCNN-24</FONT></A> [<A href="https://ieeexplore.ieee.org/abstract/document/10651072/"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>]</b></li>
		        <li>Qianqian Qi, <u>Qiming Bao</u>*, Alex Yuxuan Peng, Jiamou Liu, Michael Witbrock. <em>A Dynamic Prompt-tuning Method for Data Augmentation with Associated Knowledge</em>, <b><A href="https://openreview.net/group?id=ICLR.cc/2023/TinyPapers"><FONT face="Bitstream Vera Sans">ICLR-23 TinyPapers</FONT></A> [<A href="https://openreview.net/forum?id=hli7A0ioiS_"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>]</b></li>
			<li>Gaël Gendron, <u>Qiming Bao</u>, Michael Witbrock, Gillian Dobbie. <em>Large Language Models Are Not Strong Abstract Reasoners Yet</em>, <b><A href="https://agiworkshop.github.io/"><FONT face="Bitstream Vera Sans">AGI@ICLR 2024</FONT></A> [<A href="https://openreview.net/forum?id=Pc0fPGip78&noteId=Pc0fPGip78"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>] [<A href="https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning"><FONT face="Bitstream Vera Sans">Source code and evaluation platform</FONT></A>]</b></li>
			<li><u>Qiming Bao</u>, Lin Ni, Jiamou Liu. <em>HHH: An Online Medical Chatbot System based on Knowledge Graph and Hierarchical Bi-Directional Attention</em>, <b><A href="https://acsw.core.edu.au/2020-acsw-home"><FONT face="Bitstream Vera Sans">ACSW-20</FONT></A> [<A href="https://arxiv.org/abs/2002.03140"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>] [<A href="https://github.com/14H034160212/HHH-An-Online-Question-Answering-System-for-Medical-Questions"><FONT face="Bitstream Vera Sans">Source code</FONT></A>] [<A href="https://www.youtube.com/redirect?event=comments&redir_token=QUFFLUhqbWtqVHRmbzdQYVJFSW9odl9qZF9CWTdxUXdvQXxBQ3Jtc0ttQjRBdktkOFQ1enpvUGk1X0ZLT0hIb3g5WnhZWjVyVjFiVUQ1STdTeW9pMXdpYlJWSk9xeVA0Y01Qbm85bkQtRWxjdlk5TWdfY2I0OFNIazBhRkFoNEN6YmdjYTFaVnh3Ynkyel9LQjNhbkZ4WGxwQQ&q=https%3A%2F%2Fdocs.google.com%2Fpresentation%2Fd%2F15BfDM07IdUJiqONTAXAhIk3Y8a6oWn_2%2Fedit%3Fusp%3Dsharing%26ouid%3D116744487318855501460%26rtpof%3Dtrue%26sd%3Dtrue&stzid=UgxhkQ3dcho0vzjqWIV4AaABAg"><FONT face="Bitstream Vera Sans">Presentation slide</FONT></A>] [<A href="https://youtu.be/zTK3zZtxHs4"><FONT face="Bitstream Vera Sans">Recording</FONT></A>]</b></li>
			<!--<li>Joshua Bensemann, <u>Qiming Bao</u>, Gaël Gendron, Tim Hartill, Michael Witbrock. <em>Relating Blindsight and AI: A Review</em>, <b>Journal of Artificial Intelligence and Consciousness (2021)</b></li>-->
			<!--<li><u>Qiming Bao</u>, Michael Witbrock, Jiamou Liu. <em>From Symbolic Logic Reasoning to Soft Reasoning: A Neural-Symbolic Paradigm</em>, <b>NZAIR (2021) Workshop Poster</b></li>-->
			<li>Zhongsheng Wang, Jiamou Liu, <u>Qiming Bao</u>, Hongfei Rong, Jingfeng Zhang. <em>ChatLogic: Integrating Logic Programming with Large Language Models for Multi-step Reasoning</em>, <b><A href="https://ieeexplore.ieee.org/xpl/conhome/10649807/proceeding"><FONT face="Bitstream Vera Sans">IJCNN 2024</FONT></A> [<A href="https://ieeexplore.ieee.org/document/10650138"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>] [<A href="https://github.com/Strong-AI-Lab/ChatLogic"><FONT face="Bitstream Vera Sans">Source code</FONT></A>]</b></li>
		        <li>Zhongsheng Wang, Jiamou Liu, <u>Qiming Bao</u>, Hongfei Rong, Jingfeng Zhang. <em>ChatLogic: Integrating Logic Programming with Large Language Models for Multi-step Reasoning</em>, <b><A href="https://nuclear-workshop.github.io/"><FONT face="Bitstream Vera Sans">NucLeaR@AAAI 2024</FONT></A> [<A href="https://openreview.net/forum?id=AOqGF7Po7Z"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>] [<A href="https://github.com/Strong-AI-Lab/ChatLogic"><FONT face="Bitstream Vera Sans">Source code</FONT></A>]</b></li>
			<li>Neset TAN, Trung Nguyen, Josh Bensemann, Alex Peng, <u>Qiming Bao</u>, Yang Chen, Mark Gahegan, Michael Witbrock. <em>Multi2Claim: Generating Scientific Claims from Multi-Choice Questions for Scientific Fact-Checking</em>, <b><A href="https://2023.eacl.org/"><FONT face="Bitstream Vera Sans">EACL-23</FONT></A> [<A href="https://aclanthology.org/2023.eacl-main.194/"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>]</b></li>
			<li>Neset TAN, Alex Peng, Joshua Bensemann, <u>Qiming Bao</u>, Tim Hartill, Mark Gahegan, Michael Witbrock. <em>Input-length-shortening and text generation via attention values</em>, <b><A href="https://www.emc2-ai.org/aaai-23"><FONT face="Bitstream Vera Sans">AAAI-EMC^2-23</FONT></A> [<A href="https://arxiv.org/abs/2303.07585"><FONT face="Bitstream Vera Sans">Paper link</FONT></A>]</b></li>

        </section>
		<section>
            <h2>Work & Project Experience</h2>		
				<div class="section_sub">
					<span class="activity_header"><strong>Enhancing Max Sequence Length in Large Multimodal Models</strong></span> <span class="location">Xtracta, Auckland, New Zealand</span><br />
					<span class="activity_header activity_header_sub">Artificial Intelligence Researcher/Engineer</span> <span class="date">07/22 &ndash; now</span>
					<li>Investigated and implemented alternative attention mechanisms to extend the effective sequence length in multi-modal document processing models such as LayoutLMv3 and ERNIE-LayoutX.</li>
					<li>By applied the sliding window technique and a global attention mask from Longformer to extend the maximum sequence length from 512 to 4096, which model among LayoutLMv3 and ERNIE-LayoutX achieves a higher F1 score on the XFUND, FUNSD and other company internal datasets without significantly increasing GPU memory usage.</li>
					<li>Replicated the multi-task, multimodal pre-training code for LayoutLMv3, which Microsoft did not open source, including masked language modeling, masked image modeling, and word-patch alignment.</li>	
					<li>Integrated deepspeed and adapters into ERNIE-LayoutX and LayoutLMv3, which can reduce training costs, result in a smaller model size, and make it easier to deploy in the production environment.</li>
					<li>Successfully applied for the Research & Development Tax Incentive (RDTI) grants from Callaghan Innovation (New Zealand's Innovation Agency) for both 2022 and 2023, each offering a tax credit equal to 15% of eligible R&D expenditure. This credit can be utilised to reduce the income tax payable by the company.</li>
					<li>Integrated Flash-Attention 2 into Self-Attention can help ERNIE-LayoutX reduce maximum training GPU memory usage by up to 50% under FP16.</li>
					<li>Applied affine transformations for data augmentation to train the model and improve the robustness of line alignment issues for document extraction.</li>
					<li>By using the PEFT adapter, Flash-Attention 2 and GPTQ int4 quantization to continually train the Qwen2-VL-7B and make Qwen2-VL-7B training on a single A4090 GPU (within 24GB GPU memory).</li>
					<li>Adding page embeddings to vision-language models (Qwen2.5-VL and ERNIE-LayoutX) can improve their performance on fields that frequently appear on each page of a multi-page document (more than 15%), such as supplier names or bank names.</li>
				</div>
			<div class="section_sub">
					<span class="activity_header"><strong>Large Language Model and Logical Reasoning (Ph.D. Main Topic)</strong></span> <span class="location">UoA, Auckland, New Zealand</span><br />
					<span class="activity_header activity_header_sub">Research & Development Project Leader/Developer</span> <span class="date">02/20 &ndash; 12/24</span>
					<li>Recipient of research funding for the project Strong AI Lab (Grant No. 5000675), awarded by the Tertiary Education Commission under the Entrepreneurial Research Funding program, with a total grant amount of NZD 9.6 million. Qiming Bao was primarily responsible for the logical reasoning research direction within this project.</li>
					<li>We have developed an iterative enhancement framework based on LLM for generating explanations. The framework iteratively interacts between an explanation generation module and an explanation evaluation module to enhance the quality of the generated explanations. Our paper has been accepted by AAAI Proceedings 2025 and AGI@ICLR 2024. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/35164">paper</a> and <a href="https://github.com/Strong-AI-Lab/Explanation-Generation">source code</a>.</li>
					<li>Our method "AMR-LDA" (GPT-4 + AMR-LDA Prompt Augmentation) achieved #1 on the <a href="https://eval.ai/challenge/503/leaderboard/1347">ReClor leaderboard</a>, and we are the first group scored above 90% on the hidden test set around the world. Our paper has been accepted by the Findings of ACL-24 and LLM@IJCAI'23. <a href="https://aclanthology.org/2024.findings-acl.353/">paper</a>, <a href="https://github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning">source code</a> and <a href="https://huggingface.co/qbao775/AMR-LE-DeBERTa-V2-XXLarge-Contraposition">model weights</a>.</li>
					<li>We evaluated generative and discriminative large language models on out-of-distribution logical reasoning tasks. While they excel in standard tasks, minor changes lead to notable performance drops, indicating insufficient reasoning capabilities. Our paper has been accepted by LLM@IJCAI'23. <a href="http://arxiv.org/abs/2310.09430">paper</a> and <a href="https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning">source code</a>.</li>
					<li>To address depth imbalance in multi-step reasoning datasets and enhance model performance, we created the IMA-GloVe-GA model, combining DeepLogic with Gate Attention. Additionally, we developed a larger dataset, PARARULE-Plus, for deep multi-step reasoning over natural language. We published the <a href="https://ceur-ws.org/Vol-3212/paper15.pdf">paper</a>, <a href="https://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language">code and data</a> and <a href="http://ilp.doc.ic.ac.uk/ijclr22_videos/NeSy%20Session%205%20-%20Thursday%2029th%20-%2014_40%20-%2015_50%20(BST)%20includes%20NeSy%20Invited%20Talk%20William%20Cohen.mp4">presentation recording</a> on IJCLR-NeSy-22.</li>
					<li>We built up a dataset called AbductionRules to increase the Transformer's performance on the tasks requiring abduction reasoning. We published the <a href="https://aclanthology.org/2022.findings-acl.19/">paper</a>, <a href="https://github.com/Strong-AI-Lab/AbductionRules">code and data</a> on the Findings of ACL-22.</li>
					<li>PARARULE Plus (Multi-step deductive reasoning) and AbductionRules (Abductive reasoning) datasets are collected and merged as part of <a href="https://www.logitorch.ai/">LogiTorch.ai</a>, <a href="https://github.com/FreedomIntelligence/ReasoningNLP">ReasoningNLP</a>, <a href="https://github.com/zjunlp/Prompt4ReasoningPapers">Prompt4ReasoningPapers</a>, <a href="https://github.com/openai/evals/pull/651">OpenAI/Evals</a>, <a href="https://github.com/MLGroupJLU/LLM-eval-survey">A Survey on Evaluation of Large Language Models</a> and <a href="https://github.com/spcl/x1">Reasoning Language Models: A Blueprint</a>.</li>	
					<!--<li>We developed a model called IMA-GloVe-GA, which is based on DeepLogic and Gate Attention. It performs better performance than other RNN-based models. We published the <a href="https://www.researchgate.net/profile/Qiming-Bao/publication/356695884_From_Symbolic_Logic_Reasoning_to_Soft_Reasoning_A_Neural-Symbolic_Paradigm/links/61a80a2229948f41dbb98913/From-Symbolic-Logic-Reasoning-to-Soft-Reasoning-A-Neural-Symbolic-Paradigm.pdf">poster</a> and <a href="https://github.com/Strong-AI-Lab/A-Neural-Symbolic-Paradigm">code</a> on NZAIR-21.</li>-->
					
				</div>

                <div class="section_sub">
					<span class="activity_header"><strong>Abstract Extraction and Multi-Turn Dialogue System</strong></span> <span class="location">Advanced Institute of Information Technology, Peking University, Hangzhou, China</span><br />
					<span class="activity_header activity_header_sub">Research and Development Engineer</span> <span class="date">11/19 &ndash; 02/20</span>
					
					<li>We developed and researched a robot-based system including automatic abstract extraction, text segmentation, theme prediction, and multi-turn question answering. <!--<a href="https://14h03
					60212.github.io/portofolio/aiit/index.html">Demo link</a>.--></li>
					<li>Investigation and standard documentation of robot-related technologies.</li>
					<li>We built a well-encapsulated API to implement meeting record document processing based on the abstract extraction, text segmentation, and theme prediction.</li>
					</div>
					<div class="section_sub">
					<span class="activity_header"><strong>HHH: An Online Medical Chatbot System </strong></span> <span class="location">Precision Driven Health & Orion Health, Auckland, New Zealand</span><br />
					<span class="activity_header activity_header_sub">Research Project Leader and Developer</span> <span class="date">11/18 &ndash; 04/19</span>
					
					<li>We developed a medical text similarity algorithm called HBAM using Pre-trained Language Model and Knowledge Graph.</li>
					<li>Compared with BERT and MaLSTM models, HBAM performs higher test accuracy than the two Deep Learning models respectively <a href="https://github.com/14H034160212/HHH-An-Online-Question-Answering-System-for-Medical-Questions">code (#star: 90+)</a>, <a href="https://www.hinz.org.nz/news/461570/An-online-system-for-answering-medical-questions.htm">news</a>, <a href="https://youtu.be/zTK3zZtxHs4">recording</a> and <a href="https://arxiv.org/abs/2002.03140">published paper (#citation: 70+)</a> on ACSW-20.</li>
				</div>
        </section>
		<section>
			<h2>Invited Speaker/Visiting Scholar</h2>
			<li>Microsoft Research Asia Invited Talk 2022 (<A href="https://14h034160212.github.io/Invited_Letter_for_MSRA_Group.pdf">Invitation Letter</A>) <A href="https://14h034160212.github.io/Presentation_Slide_MSRA_Group.pdf">(Presentation Slide)</A> (<A href="https://youtu.be/nfNbSZPY4EU">Recording</A>)</li>
			<li>Samsung AI Center Cambridge UK Invited Talk 2022 <A href="https://14h034160212.github.io/Samsung_AI_Center_Cambridge_UK_Guest_Talk_Invitation_Letter.pdf">(Invitation Letter)</A> <A href="https://14h034160212.github.io/Multi_Step_Deductive_Reasoning_Over_Natural_Language_An_Empirical_Study_on_Out_of_Distribution_Generalisation_Updated_Version_Samsung.pdf">(Presentation Slide)</A> <a href="https://youtu.be/0ZkayBD3WVY">(Recording)</a></li>
			<li>IEEE Vehicular Technology Society (VTS) New Zealand North Chapter and IEEE New Zealand North Section SIGHT Group 2022 <a href="https://14h034160212.github.io/IEEE_VTS_invited_talk.jpg">(Invitation Letter)</a> <a href="https://14h034160212.github.io/Qiming_Bao_IEEE_VTS_Natural_Language_Processing_Reasoning_Invited_Talk_Final.pdf">(Presentation Slide)</a> <a href="https://youtu.be/ZzCpq5gXQto">(Recording)</a></li>
			<li>ZJU-NLP Group, Zhejiang University 2023</li>
			<li>Shanghai AI Lab 2023</li>
			<li>NLP Group, The University of Melbourne Invited Talk 2023 <a href="https://14h034160212.github.io/Invitation_UoM_NLP_Reading_Group.pdf">(Invitation Letter)</a> <a href="https://14h034160212.github.io/University_of_Melbourne_Qiming_Bao_Invited_Talk_Natural_Language_Processing_and_Reasoning.pdf">(Presentation Slide)</a></li>
			<li>Institute of Automation, Chinese Academy of Sciences Invited Talk 2023 <a href="https://14h034160212.github.io/自动化所_20240104152009.png">(Invitation Poster)</a> <a href="https://14h034160212.github.io/包启明-中国科学院自动化所“第一届紫东青年学者论坛分论坛：AI基础理论与创新应用”.pdf">(Presentation Slide)</a></li>
			<li>Shenzhen MSU-BIT University Invited Talk 2024 <a href="https://14h034160212.github.io/国际青年学者论坛人工智能研究院分论坛时间表.pdf">(Invitation Letter)</a> <a href="https://14h034160212.github.io/[深圳北理莫斯科大学] 自然语言处理和逻辑推理_20240123144811.pdf">(Presentation Slide)</a></li>
			<li>University of Massachusetts - Amherst Invited Talk 2024 <a href="https://14h034160212.github.io/UMass_Andrew_Lan_Invited_Talk.pdf">(Invitation Letter)</a> <a href="https://14h034160212.github.io/UMass_Invited_talk_Qiming_Bao.pdf">(Presentation Slide)</a></li>
			<li>Penn State University & University of Auckland Online Workshop 2024 Day 1 Session 2 Children's Future, Intercultural Learning <a href="https://14h034160212.github.io/Presentation_Confirmation_Penn_State_University_University_of_Auckland_Online_Workshop.pdf">(Invitation Letter)</a> <a href="https://14h034160212.github.io/Exploring_Iterative_Enhancement_for_Improving_Learnersourced_Multiple_Choice_Question_Explanations_with_Large_Language_Models.pdf">(Presentation Slide)</a> <a href="https://www.youtube.com/watch?v=J4YC1C2adek&list=PLVolm8QvWETzlNGRhTkx4zbm7sas0zq-5&index=3&ab_channel=PennStateGlobal">(Recording)</a></li>
			<li>Logic and AI Seminar 2025 (Peking University & Tsinghua University) invited by Prof. Fenrong Liu and A/Prof. Haoxuan Li</li>
			<li>Max Planck Institute For Software Systems invited by Prof. Adish Singla <a href="https://14h034160212.github.io/QimingBao_invitation_MPISWS_signed.pdf">(Invitation Letter)</a> </li>
			<li>Technical University of Munich invited by Dr. Stefan Fuchs</li>
		</section>
	    	<section>
			<h2>Conference Reviewer</h2>
			<li>AAAI <A href="https://aaai.org/conference/aaai/aaai-26/">2026</A> (Program Committee) (Core Rank: A*, CCF Rank: A)</li>
			<li>EMNLP <A href="https://2025.emnlp.org/">2025</A> (Reviewer) (Core Rank: A*, CCF Rank: B)</li>
			<li>COLM <A href="https://colmweb.org/">2025</A> (Reviewer) (Top LLM Conference)</li>
			<li>NAACL <A href="https://openreview.net/group?id=aclweb.org/ACL/ARR/2024/October">2024</A> (Reviewer) (Core Rank: A, CCF Rank: B)</li>
			<li>ICONIP <A href="./Review_Certificate.pdf">2024</A>/<A href="https://iconip2025.apnns.org/">2025</A> (<A href="">Program Committee</A>) (Core Rank: B, CCF Rank: C)</li>
			<li>IJCLR <A href="https://www.lamda.nju.edu.cn/ijclr24/">2024</A>, Nanjing, China (<A href="./IJCLR_2024_Certificate_of_Appreciation_Qiming_Bao.pdf">Program Committee</A>) (CCF Rank: C)</li>
			<li>NuCLeaR@AAAI <A href="https://nuclear-workshop.github.io/">2024</A>, Vancouver, Canada (<A href="https://nuclear-workshop.github.io/aaai2024/">Program Committee</A>) (Core Rank: A*, CCF Rank: A)</li>
			<li>ECAI <A href="https://ecai2023.eu/reviewer">2023</A>, Kraków, Poland (<A href="https://ecai2023.eu/reviewer">Program Committee</A>) (Core Rank: A, CCF Rank: B)</li>
			<li>ACL <A href="https://openreview.net/group?id=aclweb.org/ACL/ARR/2022/December">2022</A> (Reviewer) (Core Rank: A*, CCF Rank: A)</li>
			<li>NLPCC <A href="http://tcci.ccf.org.cn/conference/2021/nlpcc2021_handbook.pdf">2021</A>/<A href="http://tcci.ccf.org.cn/conference/2022/nlpcc2022_handbook.pdf">2022</A>/<A href="http://tcci.ccf.org.cn/conference/2023/nlpcc2023_handbook.pdf">2023</A> (Program Committee) (CCF Rank: C)</li>
		</section>
	    	<section>
			<h2>Journal Reviewer</h2>
			<li>Knowledge-based Systems <A href="https://www.sciencedirect.com/journal/knowledge-based-systems">2024</A> (SCI, IF:8.8, JCR Q1)</li>
			<li>International Journal of Artificial Intelligence in Education <A href="https://link.springer.com/journal/40593">2024</A> (SCI, IF:4.9, JCR Q1)</li>
			<li>IEEE/ACM Transactions on Computational Biology and Bioinformatics <A href="https://www.computer.org/csdl/journal/tb/2023/02/10091679/1M2I1BO6HnO">2022</A> (SCI, IF:3.71, CCF Rank B, JCR Q1)</li>
		</section>
	    	<section>
			<h2>Magazine Guest Editor</h2>
			<li><A href="http://craccum.co.nz/">CRACCUM</A>, The University of Auckland Student Magazine (<A href="https://www.craccum.co.nz/features-is-chatgpt-all-talk-and-no-action/"><FONT face="Bitstream Vera Sans">My article about whether school should allow students to use ChatGPT/GPT-4</FONT></A>)</li>
		</section>
	        <section>
			<h2>Teaching/Grant Experience and Other Achievements</h2>
			<li><A href="https://www.meetup.com/wellington-chinese-it-professionals/events/311078591/"><FONT face="Bitstream Vera Sans">Chinese IT Association NZ Guest Presenter (CITA演讲嘉宾)</FONT></A></li>
			<li>DAAD AINeT fellow 2025 on Natural Language Processing</li>
			<li><A href="https://14h034160212.github.io/shuobohui_mentor.jpg"><FONT face="Bitstream Vera Sans">Chinese Postgraduate Society Career Development Mentor (CNPG职业发展导师)</FONT></A></li>
			<li>AAAI 2025 Travel Award</li>
			<li><A href="https://14h034160212.github.io/Vice_President_of_Australia_and_New_Zealand_Alumni_Association_of_China_Jiliang_University.jpg"><FONT face="Bitstream Vera Sans">Vice President of Australia and New Zealand Alumni Association of China Jiliang University (中国计量大学澳新校友会副会长)</FONT></A></li>
			<li><A href="https://www.linkedin.com/posts/uoacompsci_introducing-qiming-bill-bao-phd-student-activity-7252066310753267713-JFKb?utm_source=share&utm_medium=member_desktop"><FONT face="Bitstream Vera Sans">Outstanding PhD student of the Faculty of Science, University of Auckland</FONT></A></li>
			<li>PhD Extension Award, University of Auckland</li>
			<li>The Computer Science Graduate Student Travel (CSGST) Award 2023 & 2024, University of Auckland</li>
	                <li>PhD Mentor (<A href="https://14h034160212.github.io/phd_mentor_certificate.jpg">Outstanding PhD Mentor</A>, School of Computer Science, University of Auckland)</li>
		        <li>The Research & Development Tax Incentive (RDTI) Grants, Callaghan Innovation (New Zealand's Innovation Agency) </li>
			<li>PhD Research Project Scholarship, University of Auckland</li>
			<li>First-Class Honours, University of Auckland</li>
			<li><A href="https://precisiondrivenhealth.com/an-online-system-for-answering-medical-questions/"><FONT face="Bitstream Vera Sans">Precision Driven Health & Orion Health Summer Scholarship</FONT></A></li>
			<li><A href="https://14h034160212.github.io/Outstanding_graduate_student_Zhejiang_Province.jpg"><FONT face="Bitstream Vera Sans">Outstanding Graduate Student</FONT></A>, Zhejiang Province (Top 1%)</li>
			<li><A href="https://www.comap-math.org/mcm/2018Certs/85922.pdf"><FONT face="Bitstream Vera Sans">The Honourable Mention of 2018 Interdisciplinary Contest In Modeling</FONT></A> (Top 10%)</li>
			<li><A href="https://mp.weixin.qq.com/s/Qs8dLEpy-2zCZeRLzuXcSQ"><FONT face="Bitstream Vera Sans">Outstanding Graduates of Hangzhou No.11 High School (杭十一中优秀毕业校友)</FONT></A></li>
			<li><A href="https://mp.weixin.qq.com/s?__biz=MzA4OTE1Nzc4MA==&mid=2651604962&idx=1&sn=d0d5630df597ee7f99d811469d3d445f&chksm=8be7de19bc90570fd5b6e5a3548c3860bd2cf040e8e8b6c4552c230c4eaf1a6c23672a809b3b&mpshare=1&srcid=072368JKCmDGsFSYtuLbiXOe&sharer_shareinfo=8b60e02a8495b330e60931cdd2130e48&sharer_shareinfo_first=8b60e02a8495b330e60931cdd2130e48&from=groupmessage&scene=1&subscene=10000&clicktime=1721697713&enterid=1721697713&sessionid=0&ascene=1&fasttmpl_type=0&fasttmpl_fullversion=7305665-en_US-zip&fasttmpl_flag=0&realreporttime=1721697713761&devicetype=android-34&version=28003145&nettype=WIFI&lang=en&countrycode=CN&exportkey=n_ChQIAhIQaU%2BrkmbEBi87n372zJvXPhL0AQIE97dBBAEAAAAAAPDII1MoEXYAAAAOpnltbLcz9gKNyK89dVj0v6GEvbkeid8ebylmZymMTX1LMOeaTJQRxj13KJqSEBYjYbMAvlh0I0oDeM7SYsGU4vZ4mXATXbiYUREPxTOGXj8bujnD6kWKYRiMnx85P5IBXphHgUe%2BGr5wbcTvdf25vqbDHLgLzrMgt%2BAcxCvjkhePM80AwtRElA6kFPcyZcJZQTv0bmR6GWVagymC6EAoWzhdgGQjMufr70m8yZ7x%2BGPrMgYSZQ0JvcSeleOe8E5WzyBn3zCQEL%2FbSaxz0penEZihWBonKC%2Bmb1no3Sw%3D&pass_ticket=AonMdaE7TOLYYVB%2F2XtfYYPKlKvstLPhuXqK4YyldNpnoIaZAYlOrtkXw7TO4ccZ&wx_header=3"><FONT face="Bitstream Vera Sans">Outstanding Graduates of China Jiliang University (中国计量大学优秀毕业校友)</FONT></A></li>
			<li><A href="./cjlu_young_scholar_forum_invitation.jpg"><FONT face="Bitstream Vera Sans">The 5th International Young Scholars Forum of China Jiliang University (中国计量大学第五届国际青年学者论坛, Top 5%)</FONT></A></li>
			<p>The University of Auckland </p>
		        <li>COMPSCI 110 Introduction to Computer Systems (Course Marker) </li>
		        <li>COMPSCI 220 Algorithms and Data Structures (Tutor) </li>
		        <li>COMPSCI 235 Software Development Methodologies (Tutor for students from both University of Auckland and Southwest University) </li>
		        <li>SOFTENG 325 Software Architecture (Tutor) </li>
		        <li>COMPSCI 367 Artificial Intelligence (Course Marker) </li>
		        <li>COMPSCI 399 Capstone: Computer Science (Tutor/Project Supervisor) </li>
		        <li>COMPSCI 703 Generalising Artificial Intelligence (Tutor/Guest Lecturer) </li>
		        <li>COMPSCI 778 Master of Information Technology Internship Mentor </li>
		        <li>Lab Demonstrator</li>
			<p>Monash University & Southeast University Joint Graduate School (Monash-SEU JGS) </p>
			<li>FIT5046 Mobile and Distributed Computing Systems (Teaching Assistant for Master's Programs)</li>
		</section>
    </div>
</body>
</html>
