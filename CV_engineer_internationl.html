<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head>
<link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>
<link href='https://fonts.googleapis.com/css?family=Open+Sans:300' rel='stylesheet' type='text/css'>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>Qiming Bao</title>
<style type="text/css">
    body {
      margin-top: 0px;
    }
    #containter{
      font-size: 11pt;
      font-family: Calibri, Candara, Segoe, "Segoe UI", Optima, Arial, sans-serif;
      line-height:12pt;
      margin-left:auto;
      margin-right:auto;
    }
    @media screen{
      #containter{
        width: 680px;
      }
    }
    .grey {
      color: #666666;
    }
    #header{
      width:100%;
      text-align: left;
      border-bottom:solid 1px #000;
    }
    a:link, a:visited{
      color: #666;
      text-decoration: underline;
    }
    p{
      margin:0;
      margin-left:1em;
    }
    .p_more_indent{
      margin:0;
      margin-left:2em;
    }
    .relevant{
      font-style:oblique;
    }
    .project_sub{
      margin:0;
      margin-left:1em;
    }
    .clear{
      clear:both;
    }
    h1{
      margin-bottom: 10pt;
      font-family: 'Open Sans Condensed', sans-serif;
      font-size: 24pt;
      line-height: 0pt;
      letter-spacing:1px;
      text-align: center;
    }
    h2{
      font-family: 'Open Sans', sans-serif;
      margin-top:0.6em;
      margin-bottom:0.4em;
      font-weight: 300;
      font-size:14pt;
      color: black;
      font-weight: 900;
    }
    ul{
      margin-top:1pt;
      margin-bottom:6pt;
    }
    li{
      margin-top:0.2em;
    }
    li strong {
      color:#878787;
    }
    .location{
      float:right;
      font-size: 10pt;
      color: #000;
    }
    .date {
      font-family: 'Open Sans', sans-serif;
      float:right;
      font-size: 10pt;
      color:#000;
    }
    #section_activities{
      /*page-break-before: always;*/
    }
    .section_sub{
      margin-bottom:10px;
    }
    .activity_header {
      font-family: 'Open Sans', sans-serif;
      color: #000;
    }
    .activity_header strong {
      font-family: 'Open Sans', sans-serif;
      font-size: 12pt;
      /*font-weight: 600;*/
      color: #000; /*#761F1F;*/
    }
    .activity_header stronger {
      font-family: 'Open Sans', sans-serif;
      font-size: 14pt;
      /*font-weight: 600;*/
      color: #000; /*#761F1F;*/
    }
    .activity_header_sub {
      font-size: 10pt;
      color: #000; /*#761F1F;*/
    }

    .header-left {
      width: 36%;
      float:left;
    }
    .header-right {
      width: 42%;
      float:right;
      text-align: right;
    }
	.header-center {
      width: 22%;
      float:right;
    }
    .small-caps {
      font-variant: small-caps;
    }
	.scholar-link {
      font-size: 9.5pt; /* Adjust font size */
      word-break: break-all; /* Enable word break */
      white-space: nowrap; /* Enable white space wrapping */
    }
    .header-item {
      display: flex;
    }
  </style>
</head><body>

<div id="containter">
<div id="header">
<div class="header-left">
<br>
<br>
Last Name: BAO<br>
First Name: QIMING<br>
<div class="header-item">
 <span class="scholar-link">Google Scholar: <a href="https://scholar.google.com/citations?user=t-PqsgcAAAAJ&hl=en">https://scholar.google.com/citations?user=t-PqsgcAAAAJ&hl=en</a></span>
</div>
</div>

<div class="header-right">
<br>
<br>
Phone: (+64) 225120099<br>
Email: bqmbill714@gmail.com<br>
GitHub: https://github.com/14H034160212<br>
</div>
<div class="header-center">
<h1>QIMING BAO</h1>
</div>
<br>
<div class="clear"></div>
</div>
<div id="content">
<!--<div class="section">
<h2>Summary/Objective</h2>
I am passionate and skilled in the efficient training and inference for large language/multimodal models, particularly in Large Language Model Instruction Fine-Tuning, Prompting, Multitask Multimodal Continual Pre-Training, word-patch alignment, and affine transformations with data augmentation for line alignment. With more than three years of work and project experience, I have practical multi-GPU parallel training experience using instruction fine-tuning with PEFT and FSDP on large language models (more than 10B parameters) such as Llama-2-13B and Llama-2-70B, and prompting large language models like ChatGPT-3.5 and GPT-4. I have experience in lora adapter-tuning and full fine-tuning with DeepSpeed, Flash-Attention 2, and FP16 on multimodal vision-language models such as LayoutLMv3 and ERNIE-LayoutX. The most representative work in my PhD is our "AMR-LDA", which has achieved #1 on the <a href="https://eval.ai/challenge/503/leaderboard/1347">ReClor leaderboard</a>, and we are the first group scored above 90% on the hidden test set around the world. The paper has been published in the Findings of ACL 2024.
</div>-->
<div class="section">
<h2>Education</h2>
<div class="section_sub">
<span class="activity_header"><strong>University of Auckland (UoA)</strong></span><span class="location">Auckland, New Zealand</span><br />
<span class="activity_header activity_header_sub">Ph.D. of Computer Science supervised by Prof. Michael Witbrock and Assoc Prof. Jiamou Liu</span>
<span class="date">02/20 &ndash; 12/24</span>
<ul>
<li>DAAD AINeT fellow 2025 on Natural Language Processing</li>
<li>PhD Research Project Scholarship/Outstanding PhD Mentor</li>
<li>Graduate Teaching Assistant (Tutor)/Research Assistant (Professional Casual Staff)</li>
<!--<li>Tutor for COMPSCI 235 Software Development Methodologies and SOFTENG 325 Software Architecture.</li>-->
</ul>
</div>
<div class="section_sub">
<span class="activity_header"><strong>University of Auckland (UoA)</strong></span><span class="location">Auckland, New Zealand</span><br />
<span class="activity_header activity_header_sub">Bachelor of Science (Honours) in Computer Science (First Class), GPA: 7/9</span>
<span class="date">07/18 &ndash; 09/19</span>
<ul>
<!--<li>Graduate Teaching Assistant & Lab Demonstrator</li>-->
<li>Precision Driven Health Summer Research Scholarship</li>
<!--<li>Class Representative of COMPSCI761 Artificial Intelligence</li>-->
<!--<li>Certificate of Outstanding Achievement in COMPSCI711 Parallel and Distributed Computing</li>-->
</ul>
</div>
	
<div class="section_sub">
<span class="activity_header"><strong>China Jiliang University (CJLU)</strong></span><span class="location">Hangzhou, China</span><br />
<span class="activity_header activity_header_sub">Bachelor of Engineering, GPA: 4.11/5</span>
<span class="date">09/14 &ndash; 07/18</span>
<ul>
<!--<li>The National Merit-Based Scholarship</li>
<li>The Zhejiang Province Government Scholarship</li>-->
<li>Outstanding Graduated Student of Zhejiang Province</li>
<!--<li>The Third Prize of Zhejiang Multimedia Competition</li>-->
<li><a href="https://www.comap-math.org/mcm/2018Certs/85922.pdf">The Honorable Mention</a> of American Mathematics Modelling Contest</li>

<!--<li>The National Merit-Based Scholarship, The Ministry of Education of the PRC</li>
<li>The Zhejiang Province Government Scholarship, The Education Department of Zhejiang Province</li>
<li>Outstanding Graduated Student of Zhejiang Province, The Education Department of Zhejiang Province</li>
<li>The Third Prize of Zhejiang Multimedia Competition, The Education Department of Zhejiang Province</li>
<li><a href="https://www.comap-math.org/mcm/2018Certs/85922.pdf">The Honorable Mention</a> of American Mathematics Modelling Contest, Mathematical Association of America</li>-->
</ul>
</div>
</div>
<div class="section">
<h2>Skills Summary</h2>
<div class="section_sub">
<ul>
<!--
<li><em>FrondEnd</em>: Manipulated with JavaScript and HTML/CSS</li>
<li><em>MobileEnd</em>: Manipulated with Android</li>
<li><em>BackEnd</em>: Manipulated with Java, Python and C#</li>
-->
<li><em>Programming Language</em>: Well manipulated with Python, Golang, Java, C#, SQL</li>
<li><em>Datastores</em>: Manipulated with Neo4j, MySQL and Microsoft SQL Server</li>
<li><em>Systems & Tools</em>: Well manipulated with Linux-based operating system and git version control</li>
<li><em>Cloud Platforms</em>: Well manipulated with Cloud GPU platforms AWS, GCP, Azure, DataCrunch and Paperspace</li>
<li><em>Machine Learning Framework</em>: Well manipulated with PyTorch, TensorFlow, Huggingface/Transformers, Lightning</li>
<li><em>Agentic AI Framework<em>: Well manipulated with Dify.ai, LangChain, CrewAI, Dumpling AI, OpenAI APIs, Syft, Xero, SharePoint</li>
<li><em>MLOps Platform</em>: Well manipulated with Weights & Biases and MLFlow</li>
<li><em>DevOps Platform</em>: Well manipulated with Hadoop, Hive and Spark.</li>
<li><em>Model Deployment</em>: Well manipulated with model lightweight deployment such as adapter</li>
<li><em>Deep Learning</em>: Well Understood with RNN, Word2Vec, LSTM, Attention, Transformer, GANs, Diffusion odels, VAEs, and Autoregressive models. Have practical multi-gpus parallel training experience using instruction fine-tuning with PEFT on large language models (more than 10B parameters) as Llama-2-13B and Llama-2-70B and Prompting large language models as ChatGPT-3.5 and GPT-4. Have experience in parameter efficient lora adapter tunning multimodal vision-language models such as InternVL2, Qwen2-VL, LayoutLMv3 and ERNIE-LayoutX with DeepSpeed, FP16 and Flash-Attention 2.</li>
<li><em>Passionate with</em>: AI/DL, NLP, Neural-Symbolic AI, Reasoning, Augmenting LLMs, Multimodal ML, RAG</li>
<li><em>Core Skills</em>: Fast Learner, Good Communication, and Self-motivated</li>
<li><em>Communication Skills</em>: Excellent communication skills and fluency in English</li>
<li><em>Machine Learning Methodologies</em>: Strong understanding of Machine Learning methodologies, including supervised learning, forecasting, recommendation systems, reinforcement learning, and multi-armed bandits</li>
<li><em>Machine Learning Experience</em>: 5+ years of Machine Learning experience</li>
</ul>
</div>
</div>
<div class="section">
<h2>Work & Project Experience</h2>
<div class="section_sub">
<span class="activity_header"><strong>Enhancing Max Sequence Length in Large Multimodal Models</strong></span> <span class="location">Xtracta, Auckland, New Zealand</span><br />
<span class="activity_header activity_header_sub">Artificial Intelligence Researcher/Engineer</span> <span class="date">07/22 &ndash; now</span>
<ul>
<li>Investigated and implemented alternative attention mechanisms to extend the effective sequence length in multi-modal document processing models such as LayoutLMv3 and ERNIE-LayoutX.</li>
<li>By applied the sliding window technique and a global attention mask from Longformer to extend the maximum sequence length from 512 to 4096, which model among LayoutLMv3 and ERNIE-LayoutX achieves a higher F1 score on the XFUND, FUNSD and other company internal datasets without significantly increasing GPU memory usage.</li>
<li>Replicated the multi-task, multimodal pre-training code for LayoutLMv3, which Microsoft did not open source, including masked language modeling, masked image modeling, and word-patch alignment.</li>	
<li>Integrated DeepSpeed and adapters into ERNIE-LayoutX and LayoutLMv3, which can reduce training costs, result in a smaller model size, and make it easier to deploy in the production environment.</li>
<li>Successfully applied for the Research & Development Tax Incentive (RDTI) grants from Callaghan Innovation (New Zealand's Innovation Agency) for both 2022 and 2023, each offering a tax credit equal to 15% of eligible R&D expenditure. This credit can be utilised to reduce the income tax payable by the company.</li>
<li>Integrated Flash-Attention 2 into Self-Attention can help ERNIE-LayoutX reduce maximum training GPU memory usage by up to 50% under FP16.</li>
<li>Applied affine transformations for data augmentation to train the model and improve the robustness of line alignment issues for document extraction.</li>
<li>By using the PEFT adapter, Flash-Attention 2 and GPTQ int4 quantization to continually train the Qwen2-VL-7B and make Qwen2-VL-7B training on a single A4090 GPU (within 24GB GPU memory).</li>
<li>Adding page embeddings to vision-language models (Qwen2.5-VL and ERNIE-LayoutX) can improve their performance on fields that frequently appear on each page of a multi-page document (more than 15%), such as supplier names or bank names.</li>
</ul>
</div>
<div class="section_sub">
<span class="activity_header"><strong>Large Language Model and Logical Reasoning (Ph.D. Main Topic)</strong></span> <span class="location">UoA, Auckland, New Zealand</span><br />
<span class="activity_header activity_header_sub">Research & Development Project Leader/Developer</span> <span class="date">02/20 &ndash; 12/24</span>
<ul>
<li>We have developed a self-reinforcement framework based on LLM for generating explanations. The framework iteratively interacts between an explanation generation module and an explanation evaluation module to enhance the quality of the generated explanations. Our paper has been accepted by AAAI Proceedings (2025) and AGI@ICLR (2024). <a href="https://ojs.aaai.org/index.php/AAAI/article/view/35164">paper</a> and <a href="https://github.com/Strong-AI-Lab/Explanation-Generation">source code</a>.</li>
<li>Our model "AMR-LDA" achieved #1 on the <a href="https://eval.ai/challenge/503/leaderboard/1347">ReClor leaderboard</a>, <a href="https://aclanthology.org/2024.findings-acl.353/">paper</a>, <a href="https://github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning">source code</a> and <a href="https://huggingface.co/qbao775/AMR-LE-DeBERTa-V2-XXLarge-Contraposition">model weights</a>. Our paper has been accepted by the Findings of ACL-24 and LLM@IJCAI'23.</li>
<li>We evaluated generative and discriminative large language models on out-of-distribution logical reasoning tasks. While they excel in standard tasks, minor changes lead to notable performance drops, indicating insufficient reasoning capabilities. Our paper has been accepted by LLM@IJCAI'23. <a href="http://arxiv.org/abs/2310.09430">paper</a> and <a href="https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning">source code</a>.</li>
<li>To address depth imbalance in multi-step reasoning datasets and enhance model performance, we created the IMA-GloVe-GA model, combining DeepLogic with Gate Attention. Additionally, we developed a larger dataset, PARARULE-Plus, for deep multi-step reasoning over natural language. We published the <a href="https://ceur-ws.org/Vol-3212/paper15.pdf">paper</a>, <a href="https://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language">code and data</a> and <a href="http://ilp.doc.ic.ac.uk/ijclr22_videos/NeSy%20Session%205%20-%20Thursday%2029th%20-%2014_40%20-%2015_50%20(BST)%20includes%20NeSy%20Invited%20Talk%20William%20Cohen.mp4">presentation recording</a> on IJCLR-NeSy-22.</li>
<li>We built up a dataset called AbductionRules to increase the Transformer's performance on the tasks requiring abduction reasoning. We published the <a href="https://aclanthology.org/2022.findings-acl.19/">paper</a>, <a href="https://github.com/Strong-AI-Lab/AbductionRules">code and data</a> on the Findings of ACL-22.</li>
<li>PARARULE Plus (Multi-step deductive reasoning) and AbductionRules (Abductive reasoning) datasets are collected and merged as part of <a href="https://www.logitorch.ai/">LogiTorch.ai</a>, <a href="https://github.com/FreedomIntelligence/ReasoningNLP">ReasoningNLP</a>, <a href="https://github.com/zjunlp/Prompt4ReasoningPapers">Prompt4ReasoningPapers</a>, <a href="https://github.com/openai/evals/pull/651">OpenAI/Evals</a>, <a href="https://github.com/MLGroupJLU/LLM-eval-survey">A Survey on Evaluation of Large Language Models</a> and <a href="https://github.com/spcl/x1">Reasoning Language Models: A Blueprint</a>.</li>	
<!--<li>We developed a model called IMA-GloVe-GA, which is based on DeepLogic and Gate Attention. It performs better performance than other RNN-based models. We published the <a href="https://www.researchgate.net/profile/Qiming-Bao/publication/356695884_From_Symbolic_Logic_Reasoning_to_Soft_Reasoning_A_Neural-Symbolic_Paradigm/links/61a80a2229948f41dbb98913/From-Symbolic-Logic-Reasoning-to-Soft-Reasoning-A-Neural-Symbolic-Paradigm.pdf">poster</a> and <a href="https://github.com/Strong-AI-Lab/A-Neural-Symbolic-Paradigm">code</a> on NZAIR-21.</li>-->
</ul>
</div>
<!--
<div class="section_sub">
<span class="activity_header"><strong>A Dynamic Knowledge Associated Prompt Tuning Method</strong></span> <span class="location">UoA, Auckland, New Zealand</span><br />
<span class="activity_header activity_header_sub">Research & Development Project Developer</span> <span class="date">09/21 &ndash; 05/22</span>
<ul>
<li>We used GPT-2 as the main backbone model and used STS-RoBERTa as a dynamic prompt which enriches the semantic information from input text to augment data. We published the <a href="https://openreview.net/pdf?id=hli7A0ioiS_">paper</a> on ICLR-23 TinyPapers.</li>
<li>We achieved better performance when adding the augmented data to finetune GPT-2 and T5 than other baseline models, including BART-paraphraser, NLPAUG, SSMBA, and static-prompt tuning method.</li>
</ul>
</div>
-->
<!--
<div class="section_sub">
<span class="activity_header"><strong>DeepQR: A Neural-based Quality Ratings Model</strong></span> <span class="location">UoA, Auckland, New Zealand</span><br />
<span class="activity_header activity_header_sub">Research & Development Project Developer</span> <span class="date">05/21 &ndash; 09/21</span>
<ul>
<li>We developed an innovative neural-based question quality evaluation model to the PeerWise platform.</li>
<li>Compared with pre-trained RoBERTa and SBERT models and other baseline models, our model shows a better question quality prediction. We published the <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21562">paper</a> on AAAI/EAAI-22.</li>
</ul>
</div>
-->
<div class="section_sub">
<span class="activity_header"><strong>Abstract Extraction and Multi-Turn Dialogue System</strong></span> <span class="location">Advanced Institute of Information Technology, Peking University, Hangzhou, China</span><br />
<span class="activity_header activity_header_sub">Research and Development Engineer</span> <span class="date">11/19 &ndash; 02/20</span>
<ul>
<li>We developed and researched a robot-based system including automatic abstract extraction, text segmentation, theme prediction, and multi-turn question answering. <!--<a href="https://14h03
60212.github.io/portofolio/aiit/index.html">Demo link</a>.--></li>
<li>Investigation and standard documentation of robot-related technologies.</li>
<li>We built a well-encapsulated API to implement meeting record document processing based on the abstract extraction, text segmentation, and theme prediction.</li>
</div>
<div class="section_sub">
<span class="activity_header"><strong>HHH: An Online Medical Chatbot System </strong></span> <span class="location">Precision Driven Health, Auckland, New Zealand</span><br />
<span class="activity_header activity_header_sub">Research Project Leader and Developer</span> <span class="date">11/18 &ndash; 04/19</span>
<ul>
<li>We developed a medical text similarity algorithm called HBAM using Deep Learning and Knowledge Graph.</li>
<li>Compared with BERT and MaLSTM models, HBAM performs higher test accuracy than the two Deep Learning models respectively <a href="https://github.com/14H034160212/HHH-An-Online-Question-Answering-System-for-Medical-Questions">code (#star: 90+)</a>, <a href="https://www.hinz.org.nz/news/461570/An-online-system-for-answering-medical-questions.htm">news</a>, <a href="https://youtu.be/zTK3zZtxHs4">recording</a> and <a href="https://arxiv.org/abs/2002.03140">published paper (#citation: 70+)</a> on ACSW-20.</li>
</div>
<!--
<div class="section_sub">
<span class="activity_header"><strong>PeerWise Question Answering Platform</strong></span> <span class="location">UoA, Auckland, New Zealand</span><br />
<span class="activity_header activity_header_sub">Research & Development Project Participant</span> <span class="date">07/18 &ndash; 07/19</span>
<ul>
<li>Developed an innovative question answering & natural language processing model to the PeerWise platform.</li>
<li>Compared with Inklebot and Logistic Regression 3-class classification model, our model shows a better question quality and answer prediction.</li>
</ul>
</div>
<div class="section_sub">
<span class="activity_header"><strong>Medical App "Healthy Mother"</strong></span> <span class="location">Service Outsourcing Laboratory, CJLU, Hangzhou, China</span><br />
<span class="activity_header activity_header_sub">Software Developer Internship</span> <span class="date">12/15 &ndash; 01/17</span>
<ul>
<li>Developed a Medical App called "Healthy Mother" based on Android <a href="https://github.com/Healthaide">github.com/Healthaide</a>.</li>
<li>Using XMPP and FPGrowth as the instant chat engine and the recommendation algorithm respectively.</li>
<li>Authorized one software copyright and pending approval one patent.</li>
</ul>
</div>
<div class="section_sub">
<span class="activity_header"><strong>Book Purchase and Management Website</strong></span> <span class="location">Service Outsourcing Laboratory, CJLU, Hangzhou, China</span><br />
<span class="activity_header activity_header_sub">Software Developer Internship</span> <span class="date">12/15 &ndash; 01/17</span>
<ul>
<li>Developed a Book Purchase and Management Website which has been included in a Chinese MOOC website.</li>
<li>Using JSP+Servlet+MySQL as the development technology.</li>
<li>Applied and authorized one software copyright.</li>
</ul>
</div>
-->
</div>

<div class="section">
<h2>Paper List</h2>
<li><u>Qiming Bao</u>, Juho Leinonen, Alex Yuxuan Peng, Wanjun Zhong, Tim Pistotti, Alice Huang, Paul Denny, Michael Witbrock, Jiamou Liu. <em>Exploring Iterative Enhancement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models</em>, <b>Proceedings of the AAAI Conference on Artificial Intelligence (2025)</b></li>
<!--<li><u>Qiming Bao</u>, Juho Leinonen, Alex Yuxuan Peng, Wanjun Zhong, Tim Pistotti, Alice Huang, Paul Denny, Michael Witbrock and Jiamou Liu) <em>Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models</em>, <b>arXiv preprint arXiv:2309.1044 (2023)</b></li>-->
<li><u>Qiming Bao</u>, Alex Peng, Zhenyun Deng, Wanjun Zhong, Gaël Gendron, Neşet Tan, Nathan Young, Yang Chen, Yonghua Zhu, Michael Witbrock, Jiamou Liu. <em>Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning.</em>, <b>The Findings of ACL (2024)</b></li>
<li><u>Qiming Bao</u>, Juho Leinonen, Alex Yuxuan Peng, Wanjun Zhong, Tim Pistotti, Alice Huang, Paul Denny, Michael Witbrock, Jiamou Liu. <em>Exploring Iterative Enhancement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models</em>, <b>AGI@ICLR (2024)</b></li>
<li><u>Qiming Bao</u>, Gaël Gendron, Alex Peng, Neset Tan, Michael Witbrock, Jiamou Liu. <em>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning.</em>, <b>ICONIP (2024)</b></li>
<li><u>Qiming Bao</u>, Gaël Gendron, Alex Peng, Neset Tan, Michael Witbrock, Jiamou Liu. <em>A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks.</em>, <b>LLM@IJCAI (2023)</b></li>
<li><u>Qiming Bao</u>, Alex Peng, Zhenyun Deng, Wanjun Zhong, Gaël Gendron, Neşet Tan, Nathan Young, Yang Chen, Yonghua Zhu, Michael Witbrock and Jiamou Liu. <em>Enhancing Logical Reasoning of Large Language Models through Logic-Driven Data Augmentation.</em>, <b>LLM@IJCAI (2023)</b></li>
<!--<li><u>Qiming Bao</u>, Alex Peng, Zhenyun Deng, Wanjun Zhong, Neset Tan, Nathan Young, Yang Chen, Yonghua Zhu, Michael Witbrock, Jiamou Liu. <em>Contrastive Learning with Logic-driven Data Augmentation for Logical Reasoning over Text.</em>, <b>arXiv preprint arXiv:2305.12599 (2023)</b></li>-->
<li><u>Qiming Bao</u>, Alex Peng, Tim Hartill, Neset Tan, Zhenyun Deng, Michael Witbrock, Jiamou Liu. <em>Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation</em>, <b>IJCLR-NeSy (2022)</b></li>
<li>Nathan Young, <u>Qiming Bao</u>, Joshua Ljudo Bensemann, Michael J. Witbrock. <em>AbductionRules: Training Transformers to Explain Unexpected Inputs</em>, <b>The Findings of ACL (2022)</b></li>
<li>Gaël Gendron, <u>Qiming Bao</u>, Michael Witbrock, Gillian Dobbie. <em>Large Language Models Are Not Strong Abstract Reasoners</em>, <b>IJCAI (2024)</b></li>
<li>Lin Ni, <u>Qiming Bao</u>, Xiaoxuan Li, Qianqian Qi, Paul Denny, Jim Warren, Michael Witbrock, Jiamou Liu. <em>DeepQR: Neural-based Quality Ratings for Learnersourced Multiple-Choice Questions</em>, <b>Proceedings of the AAAI Conference on Artificial Intelligence (2022)</b></li>
<li>Qianqian Qi, <u>Qiming Bao</u>*, Alex Yuxuan Peng, Jiamou Liu, Michael Witbrock. <em>A Dynamic Prompt-tuning Method for Data Augmentation with Associated Knowledge</em>, <b>ICLR TinyPapers (2023)</b></li>
<li>Gaël Gendron, <u>Qiming Bao</u>, Michael Witbrock, Gillian Dobbie. <em>Large Language Models Are Not Strong Abstract Reasoners Yet</em>, <b>AGI@ICLR (2024)</b></li>
<!--<li>Gaël Gendron, <u>Qiming Bao</u>, Michael Witbrock, Gillian Dobbie. <em>Large Language Models Are Not Abstract Reasoners.</em>, <b>arXiv preprint arXiv:2305.19555 (2023)</b></li>-->
<li><u>Qiming Bao</u>, Lin Ni, Jiamou Liu. <em>HHH: An Online Medical Chatbot System based on Knowledge Graph and Hierarchical Bi-Directional Attention</em>, <b>ACSW (2020)</b></li>
<!--<li>Joshua Bensemann, <u>Qiming Bao</u>, Gaël Gendron, Tim Hartill, Michael Witbrock. <em>Relating Blindsight and AI: A Review</em>, <b>Journal of Artificial Intelligence and Consciousness (2021)</b></li>-->
<!--<li><u>Qiming Bao</u>, Michael Witbrock, Jiamou Liu. <em>From Symbolic Logic Reasoning to Soft Reasoning: A Neural-Symbolic Paradigm</em>, <b>NZAIR (2021) Workshop Poster</b></li>-->
<li>Zhongsheng Wang, Jiamou Liu, <u>Qiming Bao</u>, Hongfei Rong, Jingfeng Zhang. <em>ChatLogic: Integrating Logic Programming with Large Language Models for Multi-step Reasoning</em>, <b>NucLeaR@AAAI (2024)</b></li>
<li>Neset Ozkan TAN, Trung Nguyen, Josh Bensemann, Alex Peng, <u>Qiming Bao</u>, Yang Chen, Mark Gahegan, Michael Witbrock. <em>Multi2Claim: Generating Scientific Claims from Multi-Choice Questions for Scientific Fact-Checking</em>, <b>EACL (2023)</b></li>
<li>Neset Tan, Alex Peng, Joshua Bensemann, <u>Qiming Bao</u>, Tim Hartill, Mark Gahegan, Michael Witbrock. <em>Input-length-shortening and text generation via attention values</em>, <b>AAAI-EMC^2 (2023)</b></li>
</div>
<!--
<div class="section">
<h2>Invited Speaker</h2>
<li>Microsoft Research Asia Invited Talk 2022 (<A href="https://14h034160212.github.io/Invited_Letter_for_MSRA_Group.pdf">Invitation Letter</A>) <A href="https://14h034160212.github.io/Presentation_Slide_MSRA_Group.pdf">(Presentation Slide)</A> (<A href="https://youtu.be/nfNbSZPY4EU">Recording</A>)</li>
<li>Samsung AI Center Cambridge UK Invited Talk 2022 <A href="https://14h034160212.github.io/Samsung_AI_Center_Cambridge_UK_Guest_Talk_Invitation_Letter.pdf">(Invitation Letter)</A> <A href="https://14h034160212.github.io/Multi_Step_Deductive_Reasoning_Over_Natural_Language_An_Empirical_Study_on_Out_of_Distribution_Generalisation_Updated_Version_Samsung.pdf">(Presentation Slide)</A> <a href="https://youtu.be/0ZkayBD3WVY">(Recording)</a></li>
<li>IEEE Vehicular Technology Society (VTS) New Zealand North Chapter and IEEE New Zealand North Section SIGHT Group 2022 <a href="https://14h034160212.github.io/IEEE_VTS_invited_talk.jpg">(Invitation Letter)</a> <a href="https://14h034160212.github.io/Qiming_Bao_IEEE_VTS_Natural_Language_Processing_Reasoning_Invited_Talk_Final.pdf">(Presentation Slide)</a> <a href="https://youtu.be/ZzCpq5gXQto">(Recording)</a></li>
<li>NLP Group, The University of Melbourne Invited Talk 2023 <a href="https://14h034160212.github.io/Invitation_UoM_NLP_Reading_Group.pdf">(Invitation Letter)</a> <a href="https://14h034160212.github.io/University_of_Melbourne_Qiming_Bao_Invited_Talk_Natural_Language_Processing_and_Reasoning.pdf">(Presentation Slide)</a></li>
</div>
</div>
-->
<!--
<div class="section">
<h2>References</h2>
<div class="section_sub">
<span class="activity_header"><strong>Prof. Michael Witbrock</strong></span> <span class="location">University of Auckland, Auckland, New Zealand</span><br>
<span class="activity_header activity_header_sub">Professor</span> <span class="date">My Ph.D. Main Supervisor</span><br>
<span class="activity_header activity_header_sub">LinkedIn: linkedin.com/in/witbrock/</span>
</div>
<div class="section_sub">
<span class="activity_header"><strong>Dr. Jiamou Liu</strong></span> <span class="location">University of Auckland, Auckland, New Zealand</span><br>
<span class="activity_header activity_header_sub">Senior Lecturer</span> <span class="date">My Ph.D. Co-Supervisor</span><br>
<span class="activity_header activity_header_sub">LinkedIn: linkedin.com/in/jiamou-liu-b571a412/?originalSubdomain=nz</span>
</div>
<div class="section_sub">
<span class="activity_header"><strong>A/Prof. Paul Denny</strong></span> <span class="location">University of Auckland, Auckland, New Zealand</span><br>
<span class="activity_header activity_header_sub">Associate Professor</span> <span class="date">My Honours Co-Supervisor</span><br>
<span class="activity_header activity_header_sub">Homepage: https://www.cs.auckland.ac.nz/~paul/</span>
</div>
-->
<!--
<div class="section_sub">
<span class="activity_header"><strong>Dr. Radu Nicolescu</strong></span> <span class="location">University of Auckland, Auckland, New Zealand</span><br>
<span class="activity_header activity_header_sub">Senior Lecturer</span> <span class="date">The COMPSCI 711 Course Lecturer</span><br>
<span class="activity_header activity_header_sub">Email: r.nicolescu@auckland.ac.nz</span> <span class="date">Phone: +64 9 923 6831</span>
</div>
-->
<!--
<div class="section_sub">
<span class="activity_header"><strong>Prof. Tao Wang</strong></span> <span class="location">Advanced Institute of Information Technology (AIIT), Peking University, Hangzhou, China</span><br>
<span class="activity_header activity_header_sub">Vice President</span> <span class="date">My Supervisor in AIIT, Peking University</span><br>
<span class="activity_header activity_header_sub">Homepage: http://ceca.pku.edu.cn/en/people_/faculty_/tao_wang/</span>
</div>
-->
<!--
<div class="section_sub">
<span class="activity_header"><strong>Dr. Edmond Zhang</strong></span> <span class="location">Precision Driven Health, Auckland, New Zealand</span><br>
<span class="activity_header activity_header_sub">Lead Data Scientist</span> <span class="date">Precision Driven Health Summer Research Project Coordinator</span><br>
<span class="activity_header activity_header_sub">LinkedIn: linkedin.com/in/edmond-zhang</span>
-->
<!--<span class="activity_header activity_header_sub">Email: edmondzhang82@gmail.com</span>-->
</div>
</div>
</div>
</div>
<img src="http://jobminestats.appspot.com/Ping/ag5zfmpvYm1pbmVzdGF0c3IMCxIFUGl4ZWwY0Q8M.gif" height="0" width="0" />
<script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script type="f73437069cac42d18ac8ac5c-text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-17445820-1']);
  _gaq.push(['_setDomainName', '.stephenholiday.com']);
  _gaq.push(['_trackPageview']);

  (function() {
  var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
  ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
  var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
<script src="https://ajax.cloudflare.com/cdn-cgi/scripts/a2bd7673/cloudflare-static/rocket-loader.min.js" data-cf-settings="f73437069cac42d18ac8ac5c-|49" defer=""></script></body></html>

